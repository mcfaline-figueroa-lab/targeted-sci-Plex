
# To follow this pipeline, edit environment/directory specifics, copy and paste the commands onto command line
# begin a session with 50gb memory
# srun --pty --time=08:00:00 --mem=50gb

# Define working directory where outputs and logs will be stored
WORKING_DIR=

# Define hash barcodes file and add to working directory (different for each experiment)
HASH_BARCODES_FILE=$WORKING_DIR/hashSampleSheet.txt

# Refer to scripts + references directory 
SCRIPTS_DIR=sci-RNA-seq/scripts
REF_DIR=sci-RNA-seq/reference


# STAR_INDEX is the path to the STAR genome index you want to align reads with.
# Create star indices for for the human genome 

# Used for the mapping of transcripts
# STAR_INDEX=$REF_DIR/human/
# STAR_INDEX=$REF_DIR/mouse/

# GENE_MODEL_DIR is a directory with BED files for various genomic features
# that are used to assign aligned reads to genes.

# Used as the gene model 
GENE_MODEL_DIR=$REF_DIR/gene_model/human
GENE_MODEL_DIR=$REF_DIR/gene_model/mouse

# Ligation barcode file
LIG_BARCODES_FILE=$REF_DIR/ligation_384_3lvl

# Download GNU Datamash
# https://www.gnu.org/software/datamash/
DATAMASH_PATH=

# Change input path to bin where commands are found
# as in the original sci-plex demultiplexing pipeline:
# need to download packages for cutadapt, trimgalore, samtools, bedtools
BIN=<path_to_bin>

# Change this value to the number of batches to run at one time
BATCH_SIZE=10

# Give experiment desired name
SAMPLE_NAME="sciPlex"
echo "$SAMPLE_NAME" > $WORKING_DIR/combinatorial.indexing.key

### Creates directory to put slurm batch job logs
mkdir $WORKING_DIR/batch-logs
mkdir $WORKING_DIR/batch-logs/1-R1-info-R2-logs
mkdir $WORKING_DIR/batch-logs/2-trim-logs
mkdir $WORKING_DIR/batch-logs/3-STAR-logs
mkdir $WORKING_DIR/batch-logs/4-sam-sort-logs
mkdir $WORKING_DIR/batch-logs/5-count-rRNA-logs
mkdir $WORKING_DIR/batch-logs/6-make-bed-logs
mkdir $WORKING_DIR/batch-logs/7-assign-reads-logs
mkdir $WORKING_DIR/batch-logs/8-UMI-per-sample-logs
mkdir $WORKING_DIR/batch-logs/10-UMI-rollup-logs

#-------------------------------------------------------------------------------
# Put read 1 info (RT well, UMI) into read 2 read name
# 3 Level sci-RNA Seq Specific Chunk of code
# FOR TARGETED SCI-- only need to run 2-combined-fastq (no randomN primers, no need to make 2-combined-fastq-comb,
# run step three from 2-combined-fastq
#-------------------------------------------------------------------------------

cd $WORKING_DIR
 
mkdir 2-combined-fastq
mkdir 2-combined-fastq/file-lists-for-r1-info-munging
 
ls 1-output-fastq-comb/ | grep _R1_ | grep -v Undetermined | split -l $BATCH_SIZE -d - 2-combined-fastq/file-lists-for-r1-info-munging/
   
ls 2-combined-fastq/file-lists-for-r1-info-munging | while read BATCH; do
   sbatch $SCRIPTS_DIR/2-put-read1-info-in-read2_3Level.sh \
       $WORKING_DIR/1-output-fastq-comb                                      \
       $WORKING_DIR/2-combined-fastq/file-lists-for-r1-info-munging/$BATCH      \
       $SCRIPTS_DIR/                                           \
       $RT_BARCODES_FILE                                       \
       $LIG_BARCODES_FILE                                      \
       $WORKING_DIR/combinatorial.indexing.key                 \
       $WORKING_DIR/2-combined-fastq                            
 done

mkdir 2-combined-fastq-rand
mkdir 2-combined-fastq-rand/file-lists-for-r1-info-munging


ls 1-output-fastq/ | grep _R1_ | grep -v Undetermined | split -l $BATCH_SIZE -d - 2-combined-fastq-rand/file-lists-for-r1-info-munging/
   
ls 2-combined-fastq-rand/file-lists-for-r1-info-munging | while read BATCH; do
   sbatch $SCRIPTS_DIR/2-put-read1-info-in-read2_3Level.sh \
       $WORKING_DIR/1-output-fastq/                                   \
       $WORKING_DIR/2-combined-fastq-rand/file-lists-for-r1-info-munging/$BATCH      \
       $SCRIPTS_DIR/                                           \
       $RT_BARCODES_FILE_RAND                                       \
       $LIG_BARCODES_FILE                                      \
       $WORKING_DIR/combinatorial.indexing.key                 \
       $WORKING_DIR/2-combined-fastq-rand                            
 done
#-------------------------------------------------------------------------------

#-------------------------------------------------------------------------------
# Merge two RT files

#-------------------------------------------------------------------------------
mkdir 2-combined-fastq-comb

paste <(ls 2-combined-fastq/*.fastq.gz | sort) <(ls 2-combined-fastq-rand/*.fastq.gz | sort) | awk -v output_dir=2-combined-fastq-comb/ '{split($1, a, "/"); print "cat "$1" "$2" > "output_dir"/"a[length(a)]}'

#-------------------------------------------------------------------------------
# Trim poly-A tails 
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir $WORKING_DIR/3-trimmed-fastq
mkdir $WORKING_DIR/3-trimmed-fastq/file-lists-for-trimming
#run from 2-combined-fastq if doing just targeted (no randomN), change in command below

ls 2-combined-fastq-comb | grep -v file-lists | split -l $BATCH_SIZE -d - 3-trimmed-fastq/file-lists-for-trimming/
\

ls 3-trimmed-fastq/file-lists-for-trimming | while read BATCH; do
    sbatch $SCRIPTS_DIR/3-trim-STAR.sh     \
        $WORKING_DIR/2-combined-fastq-comb                        \
        $WORKING_DIR/3-trimmed-fastq/file-lists-for-trimming/$BATCH         \
        $WORKING_DIR/3-trimmed-fastq \
        $BIN

done


#-------------------------------------------------------------------------------
# Run STAR alignment
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 4-aligned-reads

sbatch $SCRIPTS_DIR/4-STAR-alignReads.sh \
    $WORKING_DIR/3-trimmed-fastq         \
    $STAR_INDEX                          \
    $WORKING_DIR/4-aligned-reads


#-------------------------------------------------------------------------------
# Filter ambiguously-mapped reads and sort BAM files
# Also count rRNA reads
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 5-aligned-reads-filtered-sorted
mkdir 5-aligned-reads-filtered-sorted/file-lists-for-samtools-sort

ls 4-aligned-reads/ | grep "[.]Aligned[.]out[.]bam$" | split -l $BATCH_SIZE -d - 5-aligned-reads-filtered-sorted/file-lists-for-samtools-sort/

ls 5-aligned-reads-filtered-sorted/file-lists-for-samtools-sort | while read BATCH; do
    sbatch $SCRIPTS_DIR/5-samtools-filter-sort.sh    \
        $WORKING_DIR/4-aligned-reads                              \
        $WORKING_DIR/5-aligned-reads-filtered-sorted/file-lists-for-samtools-sort/$BATCH   \
        $WORKING_DIR/5-aligned-reads-filtered-sorted       \
        $BIN
done


#
# You can run these jobs in parallel to the samtools-filter-sort.sh jobs
#

mkdir 6-rRNA-read-counts

ls 5-aligned-reads-filtered-sorted/file-lists-for-samtools-sort/ | while read BATCH; do
    sbatch $SCRIPTS_DIR/6-count-rRNA-reads.sh        \
        $WORKING_DIR/4-aligned-reads                              \
        $WORKING_DIR/5-aligned-reads-filtered-sorted/file-lists-for-samtools-sort/$BATCH        \
        $GENE_MODEL_DIR/latest.rRNA.gene.regions.union.bed      \
        $WORKING_DIR/6-rRNA-read-counts         \
        $BIN
done

#-------------------------------------------------------------------------------
# Split reads in BAM files into BED intervals
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 7-aligned-reads-rmdup-split-bed
mkdir 7-aligned-reads-rmdup-split-bed/file-lists-for-rmdup

ls 5-aligned-reads-filtered-sorted/ | grep "[.]bam$" | split -l $BATCH_SIZE -d - 7-aligned-reads-rmdup-split-bed/file-lists-for-rmdup/

ls 7-aligned-reads-rmdup-split-bed/file-lists-for-rmdup | while read BATCH; do
    sbatch $SCRIPTS_DIR/7-rmdup-and-make-split-bed.sh    \
        $WORKING_DIR/5-aligned-reads-filtered-sorted                  \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed/file-lists-for-rmdup/$BATCH                    \
        $SCRIPTS_DIR/                                               \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed      \
        $BIN
done




#-------------------------------------------------------------------------------
# Assign reads to genes, using the BED files as input
#-------------------------------------------------------------------------------
####FOR GENCODE:

mkdir $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode
FILE_LIST=$(ls $WORKING_DIR/7-aligned-reads-rmdup-split-bed/ | grep -v file-lists)
FILE=($FILE_LIST)

for i in "${FILE[@]}"
 do
  awk 'OFS="\t" {$1="chr"$1; print}' $WORKING_DIR/7-aligned-reads-rmdup-split-bed/$i \
       > $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode/$i
echo "Processed $i" 
 done

cd $WORKING_DIR

mkdir 8-unique-read-to-gene-assignments
mkdir 8-unique-read-to-gene-assignments/file-lists-for-assign-reads-to-genes


ls 7-aligned-reads-rmdup-split-bed-gencode/ | grep "[.]bed$" | split -l $BATCH_SIZE -d - 8-unique-read-to-gene-assignments/file-lists-for-assign-reads-to-genes/

ls 8-unique-read-to-gene-assignments/file-lists-for-assign-reads-to-genes | while read BATCH; do
    sbatch $SCRIPTS_DIR/8-assign-reads-to-genes.sh       \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed-gencode                  \
        $WORKING_DIR/8-unique-read-to-gene-assignments/file-lists-for-assign-reads-to-genes/$BATCH    \
        $GENE_MODEL_DIR/latest.exons.bed                            \
        $GENE_MODEL_DIR/latest.genes.bed                            \
        $SCRIPTS_DIR/                                               \
        $WORKING_DIR/8-unique-read-to-gene-assignments              \
        $BIN                                                        \
        $DATAMASH_PATH
done

####FOR NON GENCODE:
mkdir 8-unique-read-to-gene-assignments
mkdir 8-unique-read-to-gene-assignments/file-lists-for-assign-reads-to-genes

ls 7-aligned-reads-rmdup-split-bed/ | grep "[.]bed$" | split -l $BATCH_SIZE -d - 8-unique-read-to-gene-assignments/file-lists-for-assign-reads-to-genes/

ls 8-unique-read-to-gene-assignments/file-lists-for-assign-reads-to-genes | while read BATCH; do
    sbatch $SCRIPTS_DIR/8-assign-reads-to-genes.sh       \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed                  \
        $WORKING_DIR/8-unique-read-to-gene-assignments/file-lists-for-assign-reads-to-genes/$BATCH    \
        $GENE_MODEL_DIR/latest.exons.bed                            \
        $GENE_MODEL_DIR/latest.genes.bed                            \
        $SCRIPTS_DIR/                                               \
        $WORKING_DIR/8-unique-read-to-gene-assignments              \
        $BIN                                                        \
        $DATAMASH_PATH
done


#-------------------------------------------------------------------------------
# Compute the duplication rate and proportion of reads that are from rRNA
#-------------------------------------------------------------------------------

cd $WORKING_DIR

mkdir 9-UMI-counts-by-sample/
mkdir 9-UMI-counts-by-sample/file-lists-for-UMI-counting

ls 7-aligned-reads-rmdup-split-bed/ | while read FILE; do
    PCR_WELL=`basename $FILE .bed`
    echo "$PCR_WELL"
done \
| split -l $BATCH_SIZE -d - 9-UMI-counts-by-sample/file-lists-for-UMI-counting/

ls 9-UMI-counts-by-sample/file-lists-for-UMI-counting | while read BATCH; do
    sbatch $SCRIPTS_DIR/9-count-UMI-per-sample.sh    \
        $WORKING_DIR/5-aligned-reads-filtered-sorted              \
        $WORKING_DIR/7-aligned-reads-rmdup-split-bed            \
        $WORKING_DIR/9-UMI-counts-by-sample/file-lists-for-UMI-counting/$BATCH         \
        $WORKING_DIR/9-UMI-counts-by-sample                       \
        $BIN                                                      \
        $DATAMASH_PATH
done

#-------------------------------------------------------------------------------

cd $WORKING_DIR
mkdir 11-final-output

cat 9-UMI-counts-by-sample/*.UMI.count | sort -k1,1 \
| $DATAMASH_PATH -g 1 sum 2 \
>11-final-output/total.UMI.count.by.sample

cat 9-UMI-counts-by-sample/*.read.count | sort -k1,1 \
| $DATAMASH_PATH -g 1 sum 2 \
>11-final-output/total.read.count.by.sample


cat 6-rRNA-read-counts/* | sort -k1,1 | $DATAMASH_PATH -g 1 sum 2 sum 3 \
| join - 11-final-output/total.UMI.count.by.sample \
| join - 11-final-output/total.read.count.by.sample \
| awk 'BEGIN {
    printf "%-18s    %11s    %8s    %10s    %8s\n",
        "sample", "n.reads", "pct.rRNA", "n.UMI", "dup.rate";
} {
    printf "%-18s    %11d    %7.1f%%    %10d    %7.1f%%\n",
        $1, $3, 100 * $2/$3, $4, 100 * (1 - $4/$5);
}' \
>11-final-output/rRNA.and.dup.rate.stats

cat 11-final-output/rRNA.and.dup.rate.stats

#-------------------------------------------------------------------------------
# Make knee plots -- Sanjay
#-------------------------------------------------------------------------------
# cd $WORKING_DIR

# START_TIME=$SECONDS
# cat unique-read-to-gene-assignments/* \
   # | /burg/home/rmg2213/.conda/envs/rmg2213_env/bin/pypy  $SCRIPTS_DIR/9-count_UMIs.py --cell - \
   # > 11-final-output/UMIs.per.cell.barcode
# ELAPSED_TIME=$(($SECONDS - $START_TIME))
# echo "Processed $FILE in $ELAPSED_TIME seconds"


# mkdir final-output/knee-plots

# Rscript $SCRIPTS_DIR/knee-plot.R            \
#    UMIs.per.cell.barcode                   \
#    $WORKING_DIR/final-output/knee-plots


#-------------------------------------------------------------------------------
# Make the final UMI count matrix
#-------------------------------------------------------------------------------
cd $WORKING_DIR

cp $GENE_MODEL_DIR/latest.gene.annotations 11-final-output/gene.annotations

mkdir 10-UMI-count-rollup
mkdir 10-UMI-count-rollup/file-lists-for-UMI-count-rollup

ls 8-unique-read-to-gene-assignments/ | grep -v file-lists | split -l $BATCH_SIZE -d - 10-UMI-count-rollup/file-lists-for-UMI-count-rollup/

ls 10-UMI-count-rollup/file-lists-for-UMI-count-rollup | while read BATCH; do
    sbatch $SCRIPTS_DIR/10-UMI-count-rollup.sh        \
        $WORKING_DIR/8-unique-read-to-gene-assignments            \
        $WORKING_DIR/10-UMI-count-rollup/file-lists-for-UMI-count-rollup/$BATCH     \
        $WORKING_DIR/10-UMI-count-rollup              \
        $DATAMASH_PATH
done

 
#-------------------------------------------------------------------------------
cd $WORKING_DIR

cat 10-UMI-count-rollup/* | gzip >prelim.UMI.count.rollup.gz

#
# Make samples.to.exclude file.
# Each line lists a sample name to exclude.
# You can leave it empty.
#

touch samples.to.exclude

#
# This uses the UMI_PER_CELL_CUTOFF variable
# defined in the knee plot section.
#

UMI_PER_CELL_CUTOFF=0
echo "UMI_PER_CELL_CUTOFF = $UMI_PER_CELL_CUTOFF"

gunzip < prelim.UMI.count.rollup.gz \
| $DATAMASH_PATH -g 1 sum 3 \
| tr '|' '\t' \
| awk -v CUTOFF=$UMI_PER_CELL_CUTOFF '
    ARGIND == 1 {
        exclude[$1] = 1;
    } $3 >= CUTOFF && !($1 in exclude) {
        print $2 "\t" $1; 
    }' samples.to.exclude - \
| sort -k1,1 -S 4G \
>11-final-output/cell.annotations

gunzip < prelim.UMI.count.rollup.gz \
| tr '|' '\t' \
| awk '{
    if (ARGIND == 1) {
        gene_idx[$1] = FNR;
    } else if (ARGIND == 2) {
        cell_idx[$1] = FNR;
    } else if ($2 in cell_idx) {
        printf "%d\t%d\t%d\n",
            gene_idx[$3], cell_idx[$2], $4;
    }
}' 11-final-output/gene.annotations 11-final-output/cell.annotations - \
>11-final-output/UMI.count.matrix

# To create a cds object without cell line annotation and without hash
# See parse hash document for parsing hash and assigning cell and hash (based on user-made files)
module load R/4.1.0
module load gdal/3.3.0
module load geos
Rscript $SCRIPTS_DIR/11-makeCDS.R \
        $WORKING_DIR/11-final-output/UMI.count.matrix \
        $WORKING_DIR/11-final-output/gene.annotations \
        $WORKING_DIR/11-final-output/cell.annotations \
        $WORKING_DIR/11-final-output

#-------------------------------------------------------------------------------
# Parse hash cs2 barcodes (only do if hash is present)
#-------------------------------------------------------------------------------
mkdir hash
mkdir hash/hashed-fastq
mkdir $WORKING_DIR/batch-logs/hash
HASH_BARCODES_FILE=$WORKING_DIR/hashSampleSheet.txt

ls 3-trimmed-fastq/file-lists-for-trimming | while read BATCH; do
    sbatch $SCRIPTS_DIR/parse_hash_cs2.sh \
        $WORKING_DIR/2-combined-fastq                          \
        $WORKING_DIR/3-trimmed-fastq/file-lists-for-trimming/$BATCH             \
        $SCRIPTS_DIR/                                           \
        $HASH_BARCODES_FILE                                     \
        $WORKING_DIR/combinatorial.indexing.key                 \
        $WORKING_DIR/hash/hashed-fastq
done
#-------------------------------------------------------------------------------
# Finish parse hash cs2 barcodes 
#-------------------------------------------------------------------------------
cd $WORKING_DIR
mkdir $WORKING_DIR/hash/hash-final

zcat hash/hashed-fastq/*.gz \
    | $DATAMASH_PATH -g 2,3,4 count 3 \
    | $DATAMASH_PATH -g 1 sum 4 \
    | awk -v S=$SAMPLE_NAME '{OFS="\t";} {print S, $0}' \
    > $WORKING_DIR/hash/hash-final/hashReads.per.cell


zcat hash/hashed-fastq/*.gz \
    | uniq \
    | $DATAMASH_PATH -g 2,3,4 count 3 \
    | $DATAMASH_PATH -g 1 sum 4 \
    | awk -v S=$SAMPLE_NAME '{OFS="\t";} {print S, $0}' \
    > $WORKING_DIR/hash/hash-final/hashUMIs.per.cell


zcat hash/hashed-fastq/*.gz \
    | uniq \
    | $DATAMASH_PATH -g 1,2,4 count 3  \
    > $WORKING_DIR/hash/hash-final/hashTable.out 

paste $WORKING_DIR/hash/hash-final/hashUMIs.per.cell  $WORKING_DIR/hash/hash-final/hashReads.per.cell \
     | cut -f 1,2,6,3 \
     | awk 'BEGIN {OFS="\t";} {dup = 1-($3/$4); print $1,$2,$3,$4,dup;}' \
     > $WORKING_DIR/hash/hash-final/hashDupRate.txt

# Some operations to look into hash stats
cat hash/hash-final/hashDupRate.txt | $DATAMASH_PATH sum 3 # tells you the UMI sum
cat hash/hash-final/hashDupRate.txt | $DATAMASH_PATH sum 4 # tells you the read sum
cat hash/hash-final/hashDupRate.txt | $DATAMASH_PATH median 5 # tells you the median duplication rate

# Post-hash counting total reads
cat hash/hash-final/hashDupRate.txt \
| $DATAMASH_PATH sum 3 sum 4 \
| awk 'BEGIN {printf "%10s  %-11s  %-10s\n", "sample", "n.reads", "n.UMI";} 
             {printf "%10s  %-12d  %-11d\n", "Hash", $2, $1;}' > hash/hash-final/hashStats.txt

cat 11-final-output-shortdT/rRNA.and.dup.rate.stats 11-final-output-randomN/rRNA.and.dup.rate.stats \
| grep -v sample \
| $DATAMASH_PATH -W -g 1 sum 2 sum 4 \
| awk 'BEGIN {} {printf "%10s  %-12d  %-11d\n", "dT+randN", $2, $3;}' \
| cat hash/hash-final/hashStats.txt - \
| awk '{for(i=2;i<=NF;i++)a[i]+=$i;print $0} END{l="SUM";i=2;while(i in a){l="   "l"  "a[i];i++};print l}' \
> 11-final-output/final.run.stats


#-------------------------------------------------------------------------------
# END OF PIPELINE
# preprocessed cds and hashTable.out can be joined and subjected to QC locally



